{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "data = []\n",
    "with open(\"./Stat628/Total.json\", encoding = \"utf-8\" ) as f:\n",
    "     for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "catagory = []\n",
    "for i in data:\n",
    "    catagory.append(i[\"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new is the index of Chinese Restautants\n",
    "new = []\n",
    "for i in catagory:\n",
    "    if i == None:\n",
    "        continue\n",
    "    if \"Chinese\" in i:\n",
    "        if (\"Restaurant\" in i) or (\"Food\" in i):\n",
    "            new.append(catagory.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinese = [data[i] for i in new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in Chinese:\n",
    "    reviews = i[\"Review\"]\n",
    "    for n in reviews:\n",
    "        text.append(n[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My girlfriend and I went for dinner at Emerald Chinese after a Thursday night workout.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sentences = []\n",
    "for review in text:\n",
    "    sentences += sent_tokenize(review)\n",
    "    \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [re.split(r\"\\W+\", sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.discard(\"not\")\n",
    "stop_words.discard(\"no\")\n",
    "stop_words.discard(\"nor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the space and the stop words\n",
    "sentences_clean = []\n",
    "for sentence in sentences:\n",
    "    cleaned = []\n",
    "    for word in sentence:\n",
    "        if (word != \"\") and (not word in stop_words):\n",
    "            cleaned.append(word.lower())\n",
    "    sentences_clean.append(cleaned)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'girlfriend',\n",
       " 'i',\n",
       " 'went',\n",
       " 'dinner',\n",
       " 'emerald',\n",
       " 'chinese',\n",
       " 'thursday',\n",
       " 'night',\n",
       " 'workout']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def getsentiscore(sentence):\n",
    "    words_tag = pos_tag(sentence)\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "    score_list=[]\n",
    "\n",
    "    tag_not_know = []\n",
    "    emotion_not_know = []\n",
    "    for idx2,t in enumerate(words_tag):\n",
    "\n",
    "        newtag=''\n",
    "\n",
    "        lemmatized=wnl.lemmatize(t[0])\n",
    "        \n",
    "        if t[1].startswith('NN'):\n",
    "\n",
    "            newtag='n'\n",
    "\n",
    "        elif t[1].startswith('JJ'):\n",
    "\n",
    "            newtag='a'\n",
    "\n",
    "        elif t[1].startswith('V'):\n",
    "\n",
    "            newtag='v'\n",
    "\n",
    "        elif t[1].startswith('R'):\n",
    "\n",
    "            newtag='r'\n",
    "\n",
    "        else:\n",
    "\n",
    "            newtag='' \n",
    "            score_list.append(0)\n",
    "            tag_not_know.append(t[0])\n",
    "\n",
    "        if(newtag!=''):    \n",
    "\n",
    "            synsets = list(swn.senti_synsets(lemmatized, newtag))\n",
    "\n",
    "            #Getting average of all possible sentiments, as you requested        \n",
    "\n",
    "            score=0\n",
    "\n",
    "            if(len(synsets)>0):\n",
    "\n",
    "                for syn in synsets:\n",
    "\n",
    "                    score+=syn.pos_score()-syn.neg_score()\n",
    "\n",
    "                score_list.append(score/len(synsets))\n",
    "            else:\n",
    "                score_list.append(0)\n",
    "                emotion_not_know.append(t[0])\n",
    "    \n",
    "    result = sum(score_list)/len(score_list)\n",
    "                \n",
    "    return result;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24 2-grams phrases\n",
    "phrases_2 = [('dim','sum'), ('fried','rice'), ('orange', 'chicken'), ('chow', 'mein'), ('pad', 'thai'), ('sour', 'soup'), ('hot', 'pot'), ('spring', 'rolls'), ('lo', 'mein'), ('bbq', 'pork'), ('mongolian', 'beef'), ('egg', 'roll'), ('wonton', 'soup'), ('soy', 'sauce'), ('ice', 'cream'), ('pork', 'belly'), ('milk', 'tea'), ('fried', 'chicken'),  ('sour', 'chicken'), ('white', 'rice'), ('sesame', 'chicken'), ('chicken', 'wings'), ('peking', 'duck'), ('bubble', 'tea')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 -grams phrases\n",
    "phrases_3 = [('egg', 'drop', 'soup'), ('beef', 'noodle', 'soup'), ('kung', 'pao', 'chicken'), ('chicken', 'fried', 'rice'), ('pork', 'fried', 'rice'), ('black', 'bean', 'sauce'), ('shrimp', 'fried', 'rice'), ('xiao', 'long', 'bao'), ('egg', 'foo', 'young'), ('dan', 'dan', 'noodles')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_score = []\n",
    "for phrase in phrases_2:\n",
    "    score = []\n",
    "    for sentence in sentences_clean:\n",
    "        index = 0\n",
    "        sentence_score = 0\n",
    "        if phrase[0] in sentence:\n",
    "            index = sentence.index(phrase[0])\n",
    "            index_need = index + 1\n",
    "            \n",
    "            #ensure the phrase[0] not the last word of sentence\n",
    "            if index_need < len(sentence):\n",
    "                if sentence[index_need] == phrase[1]:\n",
    "                    sentence_score = getsentiscore(sentence)\n",
    "                    score.append(sentence_score)\n",
    "                \n",
    "    phrases_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_2_score = []\n",
    "for i in phrases_score:\n",
    "    score = sum(i)/len(i)\n",
    "    phrases_2_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.019719092884475077,\n",
       " -5.043413935204559e-05,\n",
       " 0.005710735101401866,\n",
       " 0.020303412384346468,\n",
       " 0.013057871367722964,\n",
       " -0.02747252199597914,\n",
       " 0.009738746287431186,\n",
       " 0.012401644728690014,\n",
       " -0.007481219159794992,\n",
       " 0.021802851582992926,\n",
       " -0.008561537373841193,\n",
       " 0.00012875326863170605,\n",
       " 0.01058066672162855,\n",
       " 0.007341816131693261,\n",
       " 0.013454453279596656,\n",
       " 0.015601353462746284,\n",
       " 0.0188111275592233,\n",
       " -0.01765283659580652,\n",
       " -0.010986022022170592,\n",
       " -0.005734049523914381,\n",
       " 0.02389270223424345,\n",
       " -0.007963539008378463,\n",
       " 0.01066225336761093,\n",
       " 0.016622292236968764]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_score1 = []\n",
    "for phrase in phrases_3:\n",
    "    score = []\n",
    "    for sentence in sentences_clean:\n",
    "        index = 0\n",
    "        sentence_score = 0\n",
    "        if phrase[0] in sentence:\n",
    "            index = sentence.index(phrase[0])\n",
    "            index_need = index + 1\n",
    "            \n",
    "            #ensure the phrase[0] not the last word of sentence\n",
    "            if index_need < len(sentence):\n",
    "                if sentence[index_need] == phrase[1]:\n",
    "                    sentence_score = getsentiscore(sentence)\n",
    "                    score.append(sentence_score)\n",
    "                \n",
    "    phrases_score1.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_3_score = []\n",
    "for i in phrases_score1:\n",
    "    score = sum(i)/len(i)\n",
    "    phrases_3_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = phrases_2_score + phrases_3_score \n",
    "name = phrases_2 +phrases_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = np.argsort(score)\n",
    "name_sorted = [name[i] for i in index]\n",
    "name_sorted.reverse() # decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sesame', 'chicken'),\n",
       " ('bbq', 'pork'),\n",
       " ('chow', 'mein'),\n",
       " ('dim', 'sum'),\n",
       " ('egg', 'drop', 'soup'),\n",
       " ('milk', 'tea'),\n",
       " ('bubble', 'tea'),\n",
       " ('pork', 'belly'),\n",
       " ('ice', 'cream'),\n",
       " ('pad', 'thai'),\n",
       " ('spring', 'rolls'),\n",
       " ('kung', 'pao', 'chicken'),\n",
       " ('dan', 'dan', 'noodles'),\n",
       " ('xiao', 'long', 'bao'),\n",
       " ('peking', 'duck'),\n",
       " ('wonton', 'soup'),\n",
       " ('hot', 'pot'),\n",
       " ('soy', 'sauce'),\n",
       " ('orange', 'chicken'),\n",
       " ('beef', 'noodle', 'soup'),\n",
       " ('egg', 'roll'),\n",
       " ('fried', 'rice'),\n",
       " ('white', 'rice'),\n",
       " ('lo', 'mein'),\n",
       " ('chicken', 'wings'),\n",
       " ('mongolian', 'beef'),\n",
       " ('sour', 'chicken'),\n",
       " ('egg', 'foo', 'young'),\n",
       " ('pork', 'fried', 'rice'),\n",
       " ('fried', 'chicken'),\n",
       " ('chicken', 'fried', 'rice'),\n",
       " ('shrimp', 'fried', 'rice'),\n",
       " ('black', 'bean', 'sauce'),\n",
       " ('sour', 'soup')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=30.616121336708883, pvalue=2.597236590014156e-203)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_score = np.array(phrases_score)\n",
    "scipy.stats.ttest_ind(phrases_score[0],phrases_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
